{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Create Datasets.ipynb","provenance":[{"file_id":"1_72U-4iMEVNrAYpI6jE4rmTTwzo0qmoI","timestamp":1642426731629}],"machine_shape":"hm","collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyP2N5wBR+FHRco5w5kvTjvr"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JW8AzoSVg_ir","executionInfo":{"status":"ok","timestamp":1642994994431,"user_tz":300,"elapsed":16097,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}},"outputId":"88dd5768-669a-42c0-b3ab-3e1d6f28fa78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Imports"],"metadata":{"id":"q7IyeCj_Tt0o"}},{"cell_type":"code","source":["import numpy as np\n","import pickle\n","import os\n","import random"],"metadata":{"id":"ulF_1zfOhXOt","executionInfo":{"status":"ok","timestamp":1642995071365,"user_tz":300,"elapsed":147,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Define methods for extraction"],"metadata":{"id":"tcxKC_PYT9P3"}},{"cell_type":"code","source":["def extract_test_data( test_data_path):\n","  '''\n","  extract_test_data reads all of the spectrograms saved as npy and transforms into shape that can be used as input in training model\n","  param test_data_path: path of the test dataset\n","  return: test_data, test_label\n","  '''\n","  test_data = []\n","  test_label = []\n","  i = 0\n","  for file in os.listdir(test_data_path):\n","      if file.endswith(\".npy\"):\n","        test_data.append(os.path.join(test_data_path, file))\n","        test_label.append(int(file.split('_')[3]))\n","        i += 1\n","  print(f'There are {test_label.count(0)} interictal files')\n","  print(f'There are {test_label.count(1)} preictal files')\n","\n","  return test_data, test_label"],"metadata":{"id":"qgz-M2h1VeNj","executionInfo":{"status":"ok","timestamp":1642995073592,"user_tz":300,"elapsed":182,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["def split_training_data(train_dataset_dict, partition):\n","  '''\n","  split_training_data splits the training dataset into training and validation data sets\n","  param train_dataset_dict: dictionary with training data, blocks as keys and file paths as values\n","  param partition: percentage of data to be used for training\n","  '''\n","  ## Randomize blocks and split train:hold-out\n","  random.seed(2021)\n","  \n","  print(f'{round(partition,2)} used for training and {round(1 - partition, 2)} for validation')\n","  blocks = list(train_dataset_dict.keys())\n","  random.shuffle(blocks)\n","  # print(blocks)\n","  train_dataset_dict_shuffled = [(block, train_dataset_dict[block]) for block in blocks]\n","\n","  # split the data\n","  train_shuffled = train_dataset_dict_shuffled[:int(partition*len(train_dataset_dict_shuffled))]\n","  devtest_shuffled =  train_dataset_dict_shuffled[ int(partition*len(train_dataset_dict_shuffled)):]\n","\n","  # print(f'training data is {len(train_shuffled)} blocks, {get_num_of_files(train_shuffled)} files')\n","\n","  # print(f'devtest data is {len(devtest_shuffled)} blocks and {get_num_of_files(devtest_shuffled)} files')\n","\n","  return train_shuffled, devtest_shuffled"],"metadata":{"id":"t08Vbritajwt","executionInfo":{"status":"ok","timestamp":1642995098689,"user_tz":300,"elapsed":129,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["def extract_train_data(train_data_path):\n","  '''\n","  extract_data reads all of the spectrograms saved as npy and transforms into shape that can be used as input in training model\n","  param train_data_path: path of the training dataset\n","  return: trainDict\n","  '''\n","  trainDict = {}\n","  preictal_count = 0\n","  interictal_count = 0\n","  prevBlock = 0\n","  interictalFiles = [] \n","  preictalFiles = []\n","  i = 0\n","  for file in os.listdir(train_data_path):\n","    if file.endswith(\".npy\"):\n","      if file.split('_')[1] != '':\n","        block = int(file.split('_')[1]) # extract block\n","        trainDict[block] = {}\n","  print(f'There are {len(trainDict)} blocks in this dataset')\n","\n","  # Extract the class labels\n","  for file in os.listdir(train_data_path):\n","    if file.endswith(\".npy\"):\n","      if file.split('_')[1] != '':\n","        block = int(file.split('_')[1]) # extract block\n","        label = int(file.split('_')[3]) # extract label\n","        if label == 1: # k = 1 for preictal\n","          preictalFiles.append(os.path.join(train_data_path, file))\n","          trainDict[block][1] = preictalFiles\n","          preictal_count += 1\n","          # trainDict[block]['preictal'] = preictalFiles\n","        elif label == 0: # k = 0 for interictal\n","          interictalFiles.append(os.path.join(train_data_path, file))\n","          trainDict[block][0] = interictalFiles\n","          interictal_count += 1\n","          # trainDict[block]['interictal'] = interictalFiles\n","        if block != prevBlock:\n","          prevBlock = block # update prevBlock\n","          # reset the file list\n","          preictalFiles = [] \n","          interictalFiles = [] \n","  npys_count = preictal_count + interictal_count\n","  print(f'There are {npys_count} files in this dataset. {interictal_count} interictal, {preictal_count} preictal')\n","  return npys_count, trainDict\n","\n"],"metadata":{"id":"hrQCXQKIXJPG","executionInfo":{"status":"ok","timestamp":1642995104513,"user_tz":300,"elapsed":169,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["def create_data_list(data_dict_list):\n","  '''\n","  create_data_list creates a flat list of the dataset and extracts the labels into a separate variable\n","  param data_dict_list: a list containing a tuple of blocks and dictionary of class and path to data\n","  return: data_list, label_list\n","  '''\n","  data_list = []\n","  label_list = []\n","  # Train data list\n","  for key in data_dict_list:\n","    for k1, v1 in key[1].items():\n","      for item in v1:\n","        data_list.append(item)\n","        label_list.append(k1)\n","  print(f'There are {label_list.count(0)} interictal files')\n","  print(f'There are {label_list.count(1)} preictal files') \n","\n","  return data_list, label_list"],"metadata":{"id":"B9LVf3zCbv2c","executionInfo":{"status":"ok","timestamp":1642995108003,"user_tz":300,"elapsed":131,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Create Datasets"],"metadata":{"id":"4yTIhWnHiryQ"}},{"cell_type":"code","source":["# VALID OPTIONS ARE ['Pat1','Pat2','Pat3']\n","subId = 'Pat2'\n","train_data_path = '/content/drive/MyDrive/analysis/spectograms/'+subId+'Train_129x48_stacked'\n","test_data_path = '/content/drive/MyDrive/analysis/spectograms/'+subId+'Test_129x48_stacked'\n","\n","print('Test dataset:')\n","test_data, test_label = extract_test_data(test_data_path)\n","\n","print('')\n","print('Train Validation Split:')\n","npys_count, trainDict = extract_train_data(train_data_path)\n","partition = 1 - (len(test_data) / npys_count)\n","\n","train_shuffled, devtest_shuffled = split_training_data(trainDict, partition)\n","print('')\n","print('Training dataset:')\n","train_data, train_label = create_data_list(train_shuffled)\n","\n","print('')\n","print('Validation dataset:')\n","devtest_data, devtest_label = create_data_list(devtest_shuffled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YRaOCey-WnkI","executionInfo":{"status":"ok","timestamp":1642995371209,"user_tz":300,"elapsed":114922,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}},"outputId":"74ed562b-2e9c-43e5-8bea-94bbc8359af7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test dataset:\n","There are 2280 interictal files\n","There are 149 preictal files\n","\n","Train Validation Split:\n","There are 209 blocks in this dataset\n","There are 10007 files in this dataset. 8855 interictal, 1152 preictal\n","0.76 used for training and 0.24 for validation\n","\n","Training dataset:\n","There are 6703 interictal files\n","There are 936 preictal files\n","\n","Validation dataset:\n","There are 2029 interictal files\n","There are 198 preictal files\n"]}]},{"cell_type":"markdown","source":["## Save Labels to Drive"],"metadata":{"id":"DgGG85OQd3JO"}},{"cell_type":"code","source":["np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_train_label.npy', train_label)\n","np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_devtest_label.npy', devtest_label)\n","np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_test_label.npy', test_label)\n"],"metadata":{"id":"34qLRUIb6jGm","executionInfo":{"status":"ok","timestamp":1642996147271,"user_tz":300,"elapsed":765,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["## Extract data from path and stack into 4D arrray"],"metadata":{"id":"ZMXLVWcdd9my"}},{"cell_type":"code","source":["def stack_4D(train_data_path_list, devtest_data_path_list, test_data_path_list):\n","  '''\n","  stack_4D extracts each file in the list, stacks into a 4D array and saves to Drive\n","  param data_path_list: list of the data file paths\n","  return: None\n","  '''\n","  result = []\n","  for i in range(len(train_data_path_list)):\n","      x = np.load(train_data_path_list[i])\n","      result.append(x)\n","  train_data_4D = np.stack(result)\n","  print(np.shape(train_data_4D))\n","  np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_train_data_4D.npy', train_data_4D)\n","\n","  result = []\n","  for i in range(len(devtest_data_path_list)):\n","    x = np.load(devtest_data_path_list[i])\n","    result.append(x)\n","  devtest_data_4D = np.stack(result)\n","  print(np.shape(devtest_data_4D))\n","  np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_devtest_data_4D.npy', devtest_data_4D)\n","\n","  result = []\n","  for i in range(len(test_data_path_list)):\n","      x = np.load(test_data_path_list[i])\n","      result.append(x)\n","  test_data_4D = np.stack(result)\n","\n","  print(np.shape(test_data_4D))\n","  np.save('/content/drive/MyDrive/analysis/spectograms/cnn_model_input/'+ subId +'_data_4D.npy', test_data_4D)"],"metadata":{"id":"jF4oBWqC7Evl","executionInfo":{"status":"ok","timestamp":1642996150549,"user_tz":300,"elapsed":176,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["stack_4D(train_data, devtest_data, test_data)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RmzZFoVzgheV","executionInfo":{"status":"ok","timestamp":1642998871093,"user_tz":300,"elapsed":2718474,"user":{"displayName":"omni dragon","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"06768074288381929492"}},"outputId":"a34fe89e-8945-433d-c032-51dc8adf6cea"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["(7639, 129, 48, 16)\n","(2227, 129, 48, 16)\n","(2429, 129, 48, 16)\n"]}]}]}